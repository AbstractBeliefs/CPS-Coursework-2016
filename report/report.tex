\documentclass[a4paper]{article}

\usepackage{listings}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{fancyhdr}
\usepackage{color}
\usepackage{xcolor}
\usepackage[hidelinks]{hyperref}
\pagestyle{fancy}

\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\headwidth}{1.75in}
\addtolength{\textwidth}{1.75in}

\lhead{Concurrent and Parallel Systems}
\rhead{SET10108}
\lfoot{Coursework 1}
\cfoot{\thepage}
\rfoot{Gareth Pulham, 40099603}

\begin{document}
    \begin{titlepage}
        \title{Concurrent and Parallel Systems: Coursework 1 Report}
        \author{Gareth Pulham, 40099603}
        \date{\today}
        \maketitle
        \thispagestyle{empty}
        \begin{abstract}
            As part of Napier University's Concurrent and Parallel Systems class (SET10108), students were tasked to
            investigate and parallelise where possible a sample genetic algortihm. This document will cover one such
            investigation, covering how it was investigated and how proposed improved implementations perform in
            comparison to the original.
        \end{abstract}
    \end{titlepage}

    \tableofcontents

    \section{Introduction}
    This coursework aims to investigate the performance of a given reference program, and to evaluate and compare
    different concurrent and parallel strategies used to improve this performance.

        \subsection{Hardware}
        The development machine is a general purpose compute server, running as a virtual machine. The spec is as
        follows:
        \begin{itemize}
            \item 4$\times$ Intel(R) Xeon(R) CPU E3-1245 V2 @ 3.40GHz (virtualised)
            \item 24GB DDR3 RAM (virtualised)
            \item Debian Linux, ``Jessie'' (Linux main 3.16-3-amd64 \#1 SMP Debian 3.16.5-1 (2014-10-10) x86\_64 GNU/Linux)
        \end{itemize}

    \section{Methodology}
    Initially, time was spent in building up a set of benchmarks and profiling scripts that could be run against both
    the original, sequential program as well as any updated versions. The results of these benchmarks were used to both
    direct the improvement effort and also compare the eventual outcome of these improvements.

    These benchmarks came in two forms: profiling artifacts, and execution time benchmarks. The execution time 
    benchmarks are simply recordings of the time, on average, it takes each program variant to run. These benchmarks
    are used to identify the relative performance and the efficacy of each speedup strategy. Profiling is used to
    identify where time is spent in each variant, which can help in guiding where the programmer should spend their
    time and effort to improve performance.
    
    To generate profiling information, compiled executables are executed with the \texttt{perf} tool, which
    periodically halts and samples the current stack. These samples are later converted into flame graphs and call
    graphs. Each of these show, in essence, the same information, but in different formats that make understanding the
    flow of the program easier. Flame graphs show easily the depth of stacks, and how much time is spent in each
    function in the stack. Call graphs show caller and callee information, and also contain more numerical data about
    the runtime of each function and it's children to help back up the more intuitive interpretation of flame graphs.

    To generate runtime benchmarks, compiled executables are executed with the \texttt{time} tool, which records the
    amount of time a program runs in usermode, waiting for system calls in kernel mode, and also the wall clock time.
    In this instance, 100 iterations are executed and their wall clock times recorded. The runtimes are sorted and
    the fastest and slowest 5 runtimes are discarded. The 90 remaining execution times are then averaged to find the
    average execution time suitable for comparison with other program variants.

        \subsection{OpenMP}
        OpenMP is an API designed for multi-platform parallel programming in C/C++ (and Fortran) \cite{OpenMP}.
        OpenMP can improve program performance by allowing the programmer to easily parallelise certain situations
        with a simply, high level threading API. For example, certain \texttt{for} loops, as identified by the
        programmer, can be parallelised with the addition of a simple \texttt{\#pragma omp parallel for}.

        While this is one of the simpler cases in which OpenMP can be used, there are several configurables and
        constructs that can be used to more precisely control how data is applied to these threads. However, in this
        work we will focus mainly on scheduling.

        The OpenMP 4.0 standard \cite{OpenMP40Std} supports 3 kinds of schedule, with additional \texttt{auto} and
        \texttt{runtime} schedule keyword that delegates schedule selection to the compiler/runtime. They are as
        follows:

            \subsubsection{\texttt{static} scheduling}
            In this scheduling mode, the iterations are divided into chunks of a certain size, and these chunks are
            assigned ahead of time to each thread until none remain.

            \subsubsection{\texttt{dynamic} scheduling}
            Similar to \texttt{static}, the iterations are divided into chunks of work. Instead of being pre-assigned,
            however, these chunks are at runtime fed into a queue. Threads can then consume and work a chunk as they
            are able. By default, chunks are single iterations, meaning that 4 threads will approximately consume 25
            chunks each, however threads that are able to execute faster will consume more than slower threads,
            better smoothing the load and improving thread utilisation.

            \subsubsection{\texttt{guided} scheduling}
            \texttt{guided} scheduling is based on \texttt{dynamic}, however, the chunk size is variable within a queue
            to further improve performance by lowering overheads incurred by threads having to frequently pick new
            chunks. By having large starting chunks, all threads can begin work on most of the iterations, and threads
            that complete the fastest will later be able to pick smaller remaining chunks while slower threads catch
            up.

            \subsubsection{\texttt{runtime} and \texttt{auto} scheduling}
            These scheduling modes allow delegation of actual scheduler selection to happen later - either by the
            compiler/runtime (in the case of \texttt{auto}) or at runtime by reading the \texttt{OMP\_SCHEDULE} 
            environment variable (in the case of \texttt{runtime}).

        \subsection{C++11 Threads}
        Aside from OpenMP threading, an additional speedup strategy employed was the manual use of the threading API
        made available in the C++11 standard. This API is a more general threading API that aims to fill the same role
        as the preceding \texttt{pthread} API. Because of this, the API is more versatile, and can be used for more
        varied tasks such in the traditional concurrency models rather than just parallelised speedups. This means that
        using C++11 threads is both more complex, requiring manual thread creation, sync, and destruction, but also
        more amenable to complex workloads that may require complex inter-thread interactions.

    \section{Results}
        \subsection{Sequential Reference Implementation}
            \begin{figure}[!h]
                \centering
                \includegraphics[width=\textwidth]{{src/main.flamegraph}.png}
                \caption{Reference flame graph}
            \end{figure}
            \begin{figure}[!h]
                \centering
                \includegraphics[width=\textwidth]{{src/main.callgraph}.png}
                \caption{Reference call graph}
            \end{figure}
            Execution time: 5.077s.

            It can be seen immediately in the flame graph that most time is spent in \texttt{decode} and
            \texttt{mutate}, and reading off the execution information from the call graph does indeed show that
            while the main function spends 91\% of it's time waiting on the called function \texttt{update\_epoch},
            which mostly wraps \texttt{epoch} under source examination. \texttt{epoch} itself spends 62\% of it's
            runtime waiting on \texttt{mutate}, so these will be our speedup candidates.

        \subsection{OpenMP}
            The \texttt{epoch} and \texttt{mutate} functions were examined, and it was determined that directly
            attempting to speed up \texttt{epoch} itself would be the best first attempt. With this is mind, the main
            \texttt{for} loop was annotated with an OpenMP \texttt{parallel for} pragma. Running this alone with
            default settings produced a non-trivial performance improvement, so this was maintained and now the task is
            to identify the best settings.

            The program was adjusted so that compile time flags could easily and quickly adjust the schedule type and
            number of threads used by OpenMP. After running the OpenMP accelerated program with all schedule types and
            thread counts 1, 2, 4, 6, and 8, the following results were obtained:

            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.75\textwidth]{report/images/schedule_vs_threads.png}
                \caption{Runtimes for various schedule type and thread count}
            \end{figure}

            \begin{minipage}{\textwidth}
                \centering
                \begin{tabular}{ c c c c c c }
                    Schedule &     1 &     2 &     4 &     6 &     8 \\
                    \hline
                    \hline
                    Static   & 4.945 & 2.972 & 1.993 & 2.362 & 2.088 \\
                    Dynamic  & 4.962 & 2.964 & 1.960 & 2.091 & 2.112 \\
                    Guided   & 4.952 & 2.967 & 1.962 & 2.096 & 2.120 \\
                \end{tabular}
            \end{minipage}
            
            From this, the best performing schedule and thread count settings for this machine were found to be 4
            threads with \texttt{dynamic} scheduling. The average runtime was 1.960s.

            It can be noted that thread counts above 4 are actually detrimental to performance: this is because the
            hardware used to execute these tests has 4 hardware threads. As such, adding more threads beyond this
            cannot increase performance, as threads now compete over the right to execute in silicon. Additionally, the
            \texttt{guided} scheduler is outperformed by the \texttt{dynamic} scheduler in all multi-threaded cases.
            This is because the overhead in the \texttt{guided} scheduler does not have any additional benefits in
            cases where the load for the variois iterations is similar or the the same, but additional drawbacks.

            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.5\textwidth]{report/images/sequential_vs_omp.png}
                \caption{Runtimes for Sequential and OpenMP strategies}
            \end{figure}

            This resulted in an overall speed up of $ \frac{Sequential}{Parallel} = \frac{5.077}{1.960} = 2.59 $,
            and efficiency of $ \frac{Speedup}{Workers} = \frac{2.59}{4} = 0.65 $.

        \subsection{C++11 Threads}
            Similarly to OpenMP, speedup candidates from analysis of sequential execution were considered, and
            \texttt{epoch} was edited to spawn 4 threads to execute work. As expected, manual thread parallelisation
            provided a significant speedup, though not as much as OpenMP, and more complex to develop.

            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.5\textwidth]{report/images/sequential_vs_threading.png}
                \caption{Runtimes for Sequential and Threading strategies}
            \end{figure}

            The manual application of C++11 threads resulted in an average runtime of 2.385s.
            This results in a speedup of $ \frac{Sequential}{Parallel} = \frac{5.077}{2.385} = 2.13 $, and a resulting
            efficiency of $ \frac{Speedup}{Workers} = \frac{2.13}{4} = 0.53 $

        \subsection{Results Summary}
            \begin{figure}[!h]
                \centering
                \includegraphics[width=0.5\textwidth]{report/images/sequential_vs_all.png}
                \caption{Runtimes for all Strategies}
            \end{figure}

            \begin{minipage}{\textwidth}
                \centering
                \begin{tabular} { c c c c }
                    Strategy                    & Average Runtime & Speedup & Efficiency \\
                    \hline
                    \hline
                    Sequential                  & 5.077           & -       & -          \\
                    OpenMP (4 threads, dynamic) & 1.960           & 2.59    & 0.65       \\
                    Manual C++11 threading      & 2.385           & 2.13    & 0.53       \\
                \end{tabular}
            \end{minipage}

    \section{Conclusion}
    To conclude, this report covers the investigation of possible parallel speedup strategies applicable to a simple
    genetic algorithm, particularly, OpenMP as well as the manual application of C++11 threading.

    It can be seen from the results that even simple edits to programs to include parallelisation can have significant
    wins, very small changes with OpenMP resulting in a speedup of over 2.5x.

    \section{Biblography}
    \bibliographystyle{plain}
    \bibliography{report}

    \section{Appendices}
        \subsection{Runtime benchmarks}
        \begin{minipage}{\textwidth}
            \centering
            \begin{tabular}{ c c c c }
                Iteration & Sequential & OpenMP & Threading \\
                \hline
                \hline
                        1 &       5.05 &   1.95 &      2.40 \\
                        2 &       5.04 &   1.97 &      2.38 \\
                        3 &       5.08 &   1.99 &      2.36 \\
                        4 &       5.12 &   2.05 &      2.38 \\
                        5 &       5.08 &   1.97 &      2.39 \\
                        6 &       5.07 &   1.94 &      2.40 \\
                        7 &       5.12 &   1.95 &      2.38 \\
                        8 &       5.09 &   1.94 &      2.34 \\
                        9 &       5.10 &   1.94 &      2.38 \\
                       10 &       5.05 &   1.99 &      2.39 \\
                       11 &       5.04 &   1.94 &      2.38 \\
                       12 &       5.08 &   1.95 &      2.39 \\
                       13 &       5.07 &   1.93 &      2.36 \\
                       14 &       5.03 &   1.97 &      2.35 \\
                       15 &       5.05 &   2.11 &      2.37 \\
                       16 &       5.07 &   1.95 &      2.39 \\
                       17 &       5.05 &   1.95 &      2.35 \\
                       18 &       5.11 &   1.94 &      2.39 \\
                       19 &       5.10 &   1.95 &      2.38 \\
                       20 &       5.12 &   1.95 &      2.40 \\
                       21 &       5.12 &   1.94 &      2.33 \\
                       22 &       5.05 &   1.95 &      2.37 \\
                       23 &       5.12 &   1.94 &      2.43 \\
                       24 &       5.13 &   1.94 &      2.36 \\
                       25 &       5.07 &   1.98 &      2.35 \\
                       26 &       5.10 &   1.96 &      2.40 \\
                       27 &       5.07 &   1.95 &      2.38 \\
                       28 &       5.09 &   1.94 &      2.36 \\
                       29 &       5.08 &   1.95 &      2.38 \\
                       30 &       5.07 &   1.96 &      2.39 \\
                       31 &       5.07 &   1.94 &      2.39 \\
                       32 &       5.08 &   1.96 &      2.40 \\
                       33 &       5.04 &   2.01 &      2.40 \\
            \end{tabular}
        \end{minipage}
        
        \begin{minipage}{\textwidth}
            \centering
            \begin{tabular}{ c c c c }
                Iteration & Sequential & OpenMP & Threading \\
                \hline
                \hline
                       34 &       5.09 &   1.94 &      2.37 \\
                       35 &       5.06 &   2.00 &      2.36 \\
                       36 &       5.06 &   1.97 &      2.37 \\
                       37 &       5.06 &   1.96 &      2.37 \\
                       38 &       5.06 &   1.94 &      2.36 \\
                       39 &       5.08 &   1.95 &      2.53 \\
                       40 &       5.07 &   1.94 &      2.47 \\
                       41 &       5.07 &   1.96 &      2.40 \\
                       42 &       5.06 &   1.95 &      2.39 \\
                       43 &       5.08 &   1.95 &      2.34 \\
                       44 &       5.11 &   1.94 &      2.39 \\
                       45 &       5.12 &   2.00 &      2.40 \\
                       46 &       5.09 &   1.94 &      2.38 \\
                       47 &       5.08 &   1.97 &      2.39 \\
                       48 &       5.10 &   1.94 &      2.39 \\
                       49 &       5.06 &   1.94 &      2.37 \\
                       50 &       5.04 &   1.98 &      2.50 \\
                       51 &       5.06 &   2.11 &      2.36 \\
                       52 &       5.08 &   1.94 &      2.41 \\
                       53 &       5.09 &   1.93 &      2.35 \\
                       54 &       5.07 &   1.95 &      2.42 \\
                       55 &       5.11 &   2.01 &      2.38 \\
                       56 &       5.09 &   1.96 &      2.41 \\
                       57 &       5.07 &   1.96 &      2.41 \\
                       58 &       5.04 &   1.94 &      2.44 \\
                       59 &       5.11 &   1.95 &      2.41 \\
                       60 &       5.05 &   1.95 &      2.37 \\
                       61 &       5.10 &   1.96 &      2.40 \\
                       62 &       5.07 &   1.94 &      2.39 \\
                       63 &       5.10 &   1.98 &      2.40 \\
                       64 &       5.06 &   2.17 &      2.39 \\
                       65 &       5.07 &   1.99 &      2.37 \\
                       66 &       5.09 &   1.97 &      2.37 \\
            \end{tabular}
        \end{minipage}
        \begin{minipage}{\textwidth}
            \centering
            \begin{tabular}{ c c c c }
                Iteration & Sequential & OpenMP & Threading \\
                \hline
                \hline
                       67 &       5.10 &   1.96 &      2.41 \\
                       68 &       5.10 &   1.94 &      2.39 \\
                       69 &       5.06 &   1.95 &      2.38 \\
                       70 &       5.10 &   1.94 &      2.37 \\
                       71 &       5.14 &   1.96 &      2.40 \\
                       72 &       5.08 &   1.96 &      2.41 \\
                       73 &       5.07 &   1.94 &      2.40 \\
                       74 &       5.06 &   1.97 &      2.40 \\
                       75 &       5.11 &   1.97 &      2.41 \\
                       76 &       5.06 &   2.02 &      2.38 \\
                       77 &       5.05 &   1.94 &      2.38 \\
                       78 &       5.07 &   1.95 &      2.47 \\
                       79 &       5.09 &   2.34 &      2.35 \\
                       80 &       5.03 &   1.94 &      2.36 \\
                       81 &       5.07 &   2.01 &      2.40 \\
                       82 &       5.08 &   1.94 &      2.41 \\
                       83 &       5.06 &   1.94 &      2.46 \\
                       84 &       5.10 &   1.94 &      2.40 \\
                       85 &       5.02 &   2.02 &      2.37 \\
                       86 &       5.07 &   2.12 &      2.37 \\
                       87 &       5.02 &   1.94 &      2.36 \\
                       88 &       5.10 &   1.96 &      2.40 \\
                       89 &       5.04 &   1.94 &      2.38 \\
                       90 &       5.10 &   1.94 &      2.36 \\
                       91 &       5.07 &   1.96 &      2.37 \\
                       92 &       5.09 &   1.95 &      2.33 \\
                       93 &       5.11 &   1.95 &      2.38 \\
                       94 &       5.05 &   2.00 &      2.46 \\
                       95 &       5.06 &   1.95 &      2.41 \\
                       96 &       5.05 &   2.01 &      2.39 \\
                       97 &       5.11 &   1.99 &      2.37 \\
                       98 &       5.07 &   1.95 &      2.34 \\
                       99 &       5.07 &   1.95 &      2.37 \\
                      100 &       5.11 &   1.94 &      2.37 \\
            \end{tabular}
        \end{minipage}

\end{document}
